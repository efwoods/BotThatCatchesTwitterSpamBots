{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob\n",
    "#!pip install tweepy\n",
    "#!pip install pycountry\n",
    "#!pip install wordcloud\n",
    "#!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\n453 - You currently have Essential access which includes access to Twitter API v2 endpoints only. If you need access to this endpoint, you’ll need to apply for Elevated access via the Developer Portal. You can learn more here: https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-leve",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/10/5zg8wh6s3415f5fmtc673by80000gn/T/ipykernel_62429/3670653616.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpublic_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhome_timeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpublic_tweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpagination_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_list'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payload_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpayload_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mhome_timeline\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'since_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trim_user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exclude_replies'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                 \u001b[0;34m'include_entities'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             ), **kwargs\n\u001b[0m\u001b[1;32m    499\u001b[0m         )\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tweepy/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnauthorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mForbidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 Forbidden\n453 - You currently have Essential access which includes access to Twitter API v2 endpoints only. If you need access to this endpoint, you’ll need to apply for Elevated access via the Developer Portal. You can learn more here: https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-leve"
     ]
    }
   ],
   "source": [
    "#api = tweepy.API(auth)\n",
    "\n",
    "consumer_key = \"qJ7kqRPQn7XdxF543zqSYvl6C\"\n",
    "consumer_secret = \"jW4ggcMZLI5kcTLzfbTeX8X3Pxs22KhZgp6f1ujYWfwXbDCd2e\"\n",
    "access_token = \"1537504318496047106-HlnJeulKeYmfta0k7E1WtnYRFRINpt\"\n",
    "access_token_secret = \"SWmSHt8YODudUvhD7AFkPv8bEQA96XNYHROrdTHObqFob\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"created_at\": \"2011-12-03T03:06:19.000Z\",\n",
      "            \"description\": \"Host of Lex Fridman Podcast.\\nResearch Scientist at MIT.\\nInterested in robots and humans.\",\n",
      "            \"id\": \"427089628\",\n",
      "            \"name\": \"Lex Fridman\",\n",
      "            \"username\": \"lexfridman\"\n",
      "        },\n",
      "        {\n",
      "            \"created_at\": \"2022-06-16T18:36:30.000Z\",\n",
      "            \"description\": \"\",\n",
      "            \"id\": \"1537504318496047106\",\n",
      "            \"name\": \"Evan Woods\",\n",
      "            \"username\": \"EvanWoods\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#lookup users\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import dotenv\n",
    "\n",
    "# To set your enviornment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "dotenv.load_dotenv('.env')\n",
    "bearer_token = os.environ.get(\"BEARER_TOKEN\")\n",
    "\n",
    "\n",
    "def create_url():\n",
    "    # Specify the usernames that you want to lookup below\n",
    "    # You can enter up to 100 comma-separated values.\n",
    "    usernames = \"usernames=LexFridman,evanwoods\"\n",
    "    user_fields = \"user.fields=description,created_at\"\n",
    "    # User fields are adjustable, options include:\n",
    "    # created_at, description, entities, id, location, name,\n",
    "    # pinned_tweet_id, profile_image_url, protected,\n",
    "    # public_metrics, url, username, verified, and withheld\n",
    "    url = \"https://api.twitter.com/2/users/by?{}&{}\".format(usernames, user_fields)\n",
    "    return url\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserLookupPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url):\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth,)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = create_url()\n",
    "    json_response = connect_to_endpoint(url)\n",
    "    print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"created_at\": \"2022-11-02T17:34:25.000Z\",\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1587860694040453122\"\n",
      "            ],\n",
      "            \"id\": \"1587860694040453122\",\n",
      "            \"text\": \"Here's my conversation with Abbas Amanat, a historian at Yale specializing in the modern history of Iran. We talk about the Iran protests, the death of Mahsa Amini, abuses of power, nuclear weapons, and the history &amp; future of Iran and the Middle East. https://t.co/lder9TW90t https://t.co/fjVppSH7Ia\"\n",
      "        },\n",
      "        {\n",
      "            \"created_at\": \"2022-11-01T17:56:11.000Z\",\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1587503786515480576\"\n",
      "            ],\n",
      "            \"id\": \"1587503786515480576\",\n",
      "            \"text\": \"@elonmusk Great idea. I think you still need a state-of-the-art bot catching system for people who don't go blue. The free experience should be great as well.\"\n",
      "        },\n",
      "        {\n",
      "            \"created_at\": \"2022-11-01T17:54:46.000Z\",\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1587503430846844933\"\n",
      "            ],\n",
      "            \"id\": \"1587503430846844933\",\n",
      "            \"text\": \"@elonmusk Journalists are very mad right now.\"\n",
      "        },\n",
      "        {\n",
      "            \"created_at\": \"2022-11-01T16:00:58.000Z\",\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1587474792134893570\"\n",
      "            ],\n",
      "            \"id\": \"1587474792134893570\",\n",
      "            \"text\": \"Listen to people's pain, not their anger and hatred.\"\n",
      "        },\n",
      "        {\n",
      "            \"created_at\": \"2022-10-31T17:33:08.000Z\",\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1587135596635590657\"\n",
      "            ],\n",
      "            \"id\": \"1587135596635590657\",\n",
      "            \"text\": \"@elonmusk No one should be permanently banned.\"\n",
      "        },\n",
      "        {\n",
      "            \"created_at\": \"2022-10-31T17:07:24.000Z\",\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1587129121037074434\"\n",
      "            ],\n",
      "            \"id\": \"1587129121037074434\",\n",
      "            \"text\": \"@Rodolfoa1991 @benshapiro @AOC I would love to talk to @AOC. We'll make it happen.\"\n",
      "        },\n",
      "        {\n",
      "            \"created_at\": \"2022-10-31T16:00:46.000Z\",\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1587112350057889795\"\n",
      "            ],\n",
      "            \"id\": \"1587112350057889795\",\n",
      "            \"text\": \"I'm chatting with @benshapiro soon. If you have questions/topic suggestions, let me know.\\n\\nI will not shy away from challenging conversations with folks on the left and right, always seeking understanding through empathy, curiosity, and compassion.\\n\\nUnderstanding alleviates hate.\"\n",
      "        },\n",
      "        {\n",
      "            \"created_at\": \"2022-10-30T17:34:44.000Z\",\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1586773611549016067\"\n",
      "            ],\n",
      "            \"id\": \"1586773611549016067\",\n",
      "            \"text\": \"I'm thinking of coding up a bot that catches Twitter spam bots. Seems like a fun &amp; challenging problem to solve.\"\n",
      "        },\n",
      "        {\n",
      "            \"created_at\": \"2022-10-30T17:28:55.000Z\",\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1586772146457817088\"\n",
      "            ],\n",
      "            \"id\": \"1586772146457817088\",\n",
      "            \"text\": \"@hubermanlab @RickRubin @elonmusk And @hubermanlab is transforming science, health, and education.\\n\\nThank you for your unwavering support through thick and thin \\ud83d\\udc4a\"\n",
      "        },\n",
      "        {\n",
      "            \"created_at\": \"2022-10-30T04:09:35.000Z\",\n",
      "            \"edit_history_tweet_ids\": [\n",
      "                \"1586570990162571267\"\n",
      "            ],\n",
      "            \"id\": \"1586570990162571267\",\n",
      "            \"text\": \"RT @karpathy: Thanks Lex, I've enjoyed many of the previous episodes so it was a pleasure to come on! \\n(we've known each other from before\\u2026\"\n",
      "        }\n",
      "    ],\n",
      "    \"meta\": {\n",
      "        \"newest_id\": \"1587860694040453122\",\n",
      "        \"next_token\": \"7140dibdnow9c7btw423x55fzapesqtp4nqks7iejaf5j\",\n",
      "        \"oldest_id\": \"1586570990162571267\",\n",
      "        \"result_count\": 10\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#lookup tweets\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# To set your environment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "bearer_token = os.environ.get(\"BEARER_TOKEN\")\n",
    "\n",
    "\n",
    "def create_url(user_id = '427089628'):\n",
    "    # Replace with user ID below\n",
    "    #user_id = 2244994945\n",
    "    return \"https://api.twitter.com/2/users/{}/tweets\".format(user_id)\n",
    "\n",
    "\n",
    "def get_params():\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "    return {\"tweet.fields\": \"created_at\"}\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserTweetsPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth, params=params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = create_url()\n",
    "    params = get_params()\n",
    "    json_response = connect_to_endpoint(url, params)\n",
    "    print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "def percentage(part,whole):\n",
    " return 100 * float(part)/float(whole)\n",
    "\n",
    "\n",
    "for tweet in tweets:\n",
    "\n",
    "    #print(tweet.text)\n",
    "    tweet_list.append(tweet.text)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "    neg = score[\"neg\"]\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    " \n",
    "if neg > pos:\n",
    "    negative_list.append(tweet.text)\n",
    "    negative += 1\n",
    "elif pos > neg:\n",
    "    positive_list.append(tweet.text)\n",
    "    positive += 1\n",
    "elif pos == neg:\n",
    "    neutral_list.append(tweet.text)\n",
    "    neutral += 1\n",
    "positive = percentage(positive, noOfTweet)\n",
    "negative = percentage(negative, noOfTweet)\n",
    "neutral = percentage(neutral, noOfTweet)\n",
    "polarity = percentage(polarity, noOfTweet)\n",
    "positive = format(positive, '.1f')\n",
    "negative = format(negative, '.1f')\n",
    "neutral = format(neutral, '.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "print(“total number: “,len(tweet_list))\n",
    "print(“positive number: “,len(positive_list))\n",
    "print(“negative number: “, len(negative_list))\n",
    "print(“neutral number: “,len(neutral_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating PieCart\n",
    "labels = ['Positive ['+str(positive)+'%]' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['yellowgreen', 'blue','red']\n",
    "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "plt.style.use('default')\n",
    "plt.legend(labels)\n",
    "plt.title(“Sentiment Analysis Result for keyword= “+keyword+”” )\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Text (RT, Punctuation etc)\n",
    "#Creating new dataframe and new features\n",
    "tw_list = pd.DataFrame(tweet_list)\n",
    "tw_list[“text”] = tw_list[0]\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',” “,x)\n",
    "rt = lambda x: re.sub(“(@[A-Za-z0–9]+)|([⁰-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)”,” “,x)\n",
    "tw_list[“text”] = tw_list.text.map(remove_rt).map(rt)\n",
    "tw_list[“text”] = tw_list.text.str.lower()\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in tw_list['text'].iteritems():\n",
    " score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    " neg = score['neg']\n",
    " neu = score['neu']\n",
    " pos = score['pos']\n",
    " comp = score['compound']\n",
    " if neg > pos:\n",
    "    tw_list.loc[index, 'sentiment'] = “negative”\n",
    " elif pos > neg:\n",
    "    tw_list.loc[index, 'sentiment'] = “positive”\n",
    " else:\n",
    "    tw_list.loc[index, 'sentiment'] = “neutral”\n",
    "    tw_list.loc[index, 'neg'] = neg\n",
    "    tw_list.loc[index, 'neu'] = neu\n",
    "    tw_list.loc[index, 'pos'] = pos\n",
    "    tw_list.loc[index, 'compound'] = comp\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new data frames for all sentiments (positive, negative and neutral)\n",
    "tw_list_negative = tw_list[tw_list[“sentiment”]==”negative”]\n",
    "tw_list_positive = tw_list[tw_list[“sentiment”]==”positive”]\n",
    "tw_list_neutral = tw_list[tw_list[“sentiment”]==”neutral”]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    " total=data.loc[:,feature].value_counts(dropna=False)\n",
    " percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    " return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "#Count_values for sentiment\n",
    "count_values_in_column(tw_list,”sentiment”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for Pie Chart\n",
    "pichart = count_values_in_column(tw_list,”sentiment”)\n",
    "names= pc.index\n",
    "size=pc[“Percentage”]\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color='white')\n",
    "plt.pie(size, labels=names, colors=['green','blue','red'])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Create Wordcloud\n",
    "def create_wordcloud(text):\n",
    " mask = np.array(Image.open(“cloud.png”))\n",
    " stopwords = set(STOPWORDS)\n",
    " wc = WordCloud(background_color=”white”,\n",
    " mask = mask,\n",
    " max_words=3000,\n",
    " stopwords=stopwords,\n",
    " repeat=True)\n",
    " wc.generate(str(text))\n",
    " wc.to_file(“wc.png”)\n",
    " print(“Word Cloud Saved Successfully”)\n",
    " path=”wc.png”\n",
    " display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for all tweets\n",
    "create_wordcloud(tw_list[“text”].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for positive sentiment\n",
    "create_wordcloud(tw_list_positive[“text”].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for negative sentiment\n",
    "create_wordcloud(tw_list_negative[“text”].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating tweet's lenght and word count\n",
    "tw_list['text_len'] = tw_list['text'].astype(str).apply(len)\n",
    "tw_list['text_word_count'] = tw_list['text'].apply(lambda x: len(str(x).split()))\n",
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_len.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tw_list.groupby(“sentiment”).text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    " text = “”.join([char for char in text if char not in string.punctuation])\n",
    " text = re.sub('[0–9]+', '', text)\n",
    " return text\n",
    "tw_list['punct'] = tw_list['text'].apply(lambda x: remove_punct(x))\n",
    "#Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "tw_list['tokenized'] = tw_list['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "#Appliyng Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: stemming(x))\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "tw_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum())\n",
    "countdf = count.sort_values(0,ascending=False).head(20)\n",
    "countdf[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ngram\n",
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    " vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n",
    " bag_of_words = vec.transform(corpus)\n",
    " sum_words = bag_of_words.sum(axis=0) \n",
    " words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    " words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    " return words_freq[:n]\n",
    "#n2_bigram\n",
    "n2_bigrams = get_top_n_gram(tw_list['text'],(2,2),20)\n",
    "n2_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n3_trigram\n",
    "n3_trigrams = get_top_n_gram(tw_list['text'],(3,3),20)\n",
    "n3_trigrams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "307b3a2da12af592b81fc4bc6ba560113283ae5858fa50090e37f271e0018a6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
